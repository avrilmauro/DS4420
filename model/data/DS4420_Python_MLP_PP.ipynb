{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3c18886-8625-4d6d-81ea-1a27713ec994",
   "metadata": {},
   "source": [
    "# DS 4420 Final Project: Data Preprocessing for MLP\n",
    "This file prepares the data to be used for MLP (generates test and train sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8a54a-2888-4b41-940c-4fe50a9b00f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735cc9fb-ea30-4dab-97f2-1f4bb4671427",
   "metadata": {},
   "source": [
    "## 1. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ba1212-8ec0-444d-be60-b2bdd2dcbe52",
   "metadata": {},
   "source": [
    "**a. Cleaning User Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0d273a-893e-40ed-8ee5-85e98a4cfc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "users = pd.read_csv('PP_users.csv')\n",
    "\n",
    "# check shape\n",
    "print('Shape:', users.shape)\n",
    "\n",
    "# check for duplicate user id's\n",
    "print('\\nDuplicates:', sum(users.u.value_counts() > 1))\n",
    "\n",
    "# check for nulls\n",
    "print('\\nNull Values:')\n",
    "print(users.isna().astype(int).sum())\n",
    "\n",
    "# dropping arbitrary columns\n",
    "print('\\nDropped Columns: n_items, techniques')\n",
    "users.drop(['n_items', 'techniques'], axis=1, inplace=True)\n",
    "\n",
    "# rename columns for consistency\n",
    "users.rename(columns={'u':'user_id', 'items':'recipes'}, inplace=True)\n",
    "\n",
    "# check datatypes\n",
    "print('\\nDatatypes:')\n",
    "print(users.dtypes)\n",
    "\n",
    "# display final shape\n",
    "print('\\nUsers Shape:', users.shape)\n",
    "print('\\n')\n",
    "\n",
    "users.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a55fb-099b-403e-b829-37688d023994",
   "metadata": {},
   "source": [
    "**b. Cleaning Recipe Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97c340-5395-447b-8520-62504efbd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "recipes = pd.read_csv('RAW_recipes.csv')\n",
    "\n",
    "# check shape\n",
    "print('Shape:', recipes.shape)\n",
    "\n",
    "# check for duplicate user id's\n",
    "print('\\nDuplicates:', sum(recipes.id.value_counts() > 1))\n",
    "\n",
    "# check for nulls\n",
    "print('\\nNull Values:')\n",
    "print(recipes.isna().astype(int).sum())\n",
    "\n",
    "# drop null name\n",
    "recipes.dropna(subset='name', inplace=True)\n",
    "\n",
    "# rename columns for consistency\n",
    "recipes.rename(columns={'id':'recipe_id'}, inplace=True)\n",
    "\n",
    "# apply list transformations for relevant columns\n",
    "recipes['tags'] = recipes['tags'].apply(literal_eval)\n",
    "\n",
    "# check datatypes\n",
    "print('\\nDatatypes:')\n",
    "print(recipes.dtypes)\n",
    "\n",
    "# display final shape\n",
    "print('\\nRecipes Shape:', recipes.shape)\n",
    "print('\\n')\n",
    "\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d46cc-0d23-47b5-a8f1-9dcc481d7d4b",
   "metadata": {},
   "source": [
    "**c. Cleaning Interactions Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6796a0d-2713-4516-bf67-7eb957934042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "interactions = pd.read_csv('RAW_interactions.csv')\n",
    "\n",
    "# check shape\n",
    "print('Shape:', interactions.shape)\n",
    "\n",
    "# check for duplicate user id's\n",
    "print('\\nDuplicates:', sum(interactions.groupby(['user_id','recipe_id']).count().reset_index()[['user_id','recipe_id']].value_counts() > 1))\n",
    "\n",
    "# check for nulls\n",
    "print('\\nNull Values:')\n",
    "print(interactions.isna().astype(int).sum())\n",
    "\n",
    "# check datatypes\n",
    "print('\\nDatatypes:')\n",
    "print(interactions.dtypes)\n",
    "\n",
    "# display final shape\n",
    "print('\\nInteractions Shape:', interactions.shape)\n",
    "print('\\n')\n",
    "\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d27d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2b83886-205d-4ad9-925c-853b50f6fbaf",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c69d97-d136-4d77-bbae-52e3eb7f1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare ratings data\n",
    "ratings = interactions.drop(['review'], axis=1)  # keep date for seasonality features\n",
    "\n",
    "# extract seasonality features from date\n",
    "ratings['date'] = pd.to_datetime(ratings['date'])\n",
    "ratings['month'] = ratings['date'].dt.month\n",
    "ratings['day_of_week'] = ratings['date'].dt.dayofweek\n",
    "ratings['is_weekend'] = (ratings['day_of_week'] >= 5).astype(int)\n",
    "ratings['year'] = ratings['date'].dt.year  # fixed typo from 'fratings' to 'ratings'\n",
    "\n",
    "# merge datasets\n",
    "recipes_mlp = pd.merge(recipes, ratings, on='recipe_id', how='right')\n",
    "\n",
    "# remove duplicates\n",
    "initial_rows = len(recipes_mlp)\n",
    "recipes_mlp = recipes_mlp.drop_duplicates(subset=['user_id', 'recipe_id'])\n",
    "print(f\"removed {initial_rows - len(recipes_mlp)} duplicate ratings\")\n",
    "\n",
    "# keep only last 3 years of data\n",
    "latest_date = recipes_mlp[\"year\"].max()\n",
    "three_years_ago = latest_date - 3\n",
    "print(\"dataset latest date:\", latest_date)\n",
    "print(\"dataset 3 years prior to latest date:\", three_years_ago)\n",
    "recipes_mlp = recipes_mlp[recipes_mlp[\"year\"] > three_years_ago]\n",
    "recipes_mlp.reset_index(drop=True, inplace=True)\n",
    "print(\"filtered dataset shape:\", recipes_mlp.shape)\n",
    "\n",
    "# plot distribution of dates\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(recipes_mlp['date'].dt.year, bins=3)\n",
    "plt.title('Distribution of Ratings by Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "recipes_mlp.drop(['name', 'contributor_id', 'nutrition', 'submitted', 'description', 'steps'], axis=1, inplace=True)\n",
    "recipes_mlp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c34c8-9ab5-4d34-ac08-f68453d92bcb",
   "metadata": {},
   "source": [
    "**Balancing & Filtering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca5386-29e9-4283-a318-4f2616d11c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check rating distribution for balancing\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(recipes_mlp['rating'], bins=5)\n",
    "plt.title('Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# create a more balanced dataset with undersampling\n",
    "rating_counts = recipes_mlp['rating'].value_counts().sort_index()\n",
    "print(\"original rating counts:\", rating_counts)\n",
    "\n",
    "# find reasonable target count for each rating\n",
    "min_counts = [count for count in rating_counts.values if count > 1000]\n",
    "if min_counts:\n",
    "    target_count = min(min(min_counts), 50000)\n",
    "else:\n",
    "    target_count = 10000\n",
    "\n",
    "print(f\"target count per rating: {target_count}\")\n",
    "\n",
    "# balance ratings\n",
    "balanced_df = []\n",
    "for rating in range(1, 6):\n",
    "    if rating in rating_counts.index:\n",
    "        if rating_counts[rating] > target_count:\n",
    "            # undersample\n",
    "            balanced_df.append(recipes_mlp[recipes_mlp['rating'] == rating].sample(target_count))\n",
    "            print(f\"sampled {target_count} from rating {rating}\")\n",
    "        else:\n",
    "            # keep all\n",
    "            balanced_df.append(recipes_mlp[recipes_mlp['rating'] == rating])\n",
    "            print(f\"kept all {rating_counts[rating]} examples of rating {rating}\")\n",
    "\n",
    "recipes_mlp = pd.concat(balanced_df)\n",
    "\n",
    "# show balanced distribution\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.hist(recipes_mlp['rating'], bins=5)\n",
    "plt.title('Balanced Rating Distribution')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# map id's\n",
    "user_map = {id: i for i, id in enumerate(recipes_mlp['user_id'].unique())}\n",
    "recipe_map = {id: i for i, id in enumerate(recipes_mlp['recipe_id'].unique())}\n",
    "recipes_mlp['user_id_mapped'] = recipes_mlp['user_id'].map(user_map)\n",
    "recipes_mlp['recipe_id_mapped'] = recipes_mlp['recipe_id'].map(recipe_map)\n",
    "\n",
    "# identify users with enough ratings\n",
    "user_counts = recipes_mlp['user_id'].value_counts()\n",
    "valid_users = user_counts[user_counts >= 2].index\n",
    "\n",
    "# filter to include only valid users\n",
    "recipes_mlp_filtered = recipes_mlp[recipes_mlp['user_id'].isin(valid_users)]\n",
    "recipes_mlp_filtered.head()\n",
    "\n",
    "print(recipes_mlp_filtered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9161f93-45ca-4bec-be74-b0f7d9a9687b",
   "metadata": {},
   "source": [
    "**Train Test Split & Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b9215-c097-49c1-b548-4cf178ae86d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split, stratify by rating\n",
    "train_df, test_df = train_test_split(\n",
    "    recipes_mlp_filtered,\n",
    "    test_size=0.2,\n",
    "    stratify=recipes_mlp_filtered['rating'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# scale numeric variables, fit using the train set\n",
    "scaler_numeric = StandardScaler()\n",
    "numeric = ['minutes', 'n_steps', 'n_ingredients', 'month', 'day_of_week', 'year']\n",
    "train_df[numeric] = scaler_numeric.fit_transform(train_df[numeric])\n",
    "test_df[numeric] = scaler_numeric.transform(test_df[numeric])\n",
    "\n",
    "# parse nutrition data\n",
    "n_lists = [n.strip('[]').split(',') for n in recipes.nutrition]\n",
    "nutrition_array = np.array([\n",
    "    [float(item.strip()) for item in sublist] \n",
    "    for sublist in n_lists\n",
    "])\n",
    "\n",
    "# create nutrition dataframe\n",
    "nutrition_df = pd.DataFrame(\n",
    "    nutrition_array, \n",
    "    columns=['calories', 'total_fat', 'sugar', 'sodium', 'protein', 'saturated_fat', 'carbs'],\n",
    "    index=recipes.recipe_id\n",
    ")\n",
    "\n",
    "# scale nutrition features\n",
    "scaler_nutrition = StandardScaler()\n",
    "train_nutrition = pd.merge(train_df[['recipe_id']], nutrition_df, left_on='recipe_id', right_index=True, how='left')\n",
    "test_nutrition = pd.merge(test_df[['recipe_id']], nutrition_df, left_on='recipe_id', right_index=True, how='left')\n",
    "\n",
    "# fit scaler only on training data\n",
    "train_nutrition_values = scaler_nutrition.fit_transform(train_nutrition.drop('recipe_id', axis=1))\n",
    "test_nutrition_values = scaler_nutrition.transform(test_nutrition.drop('recipe_id', axis=1))\n",
    "\n",
    "# convert back to dataframe\n",
    "train_nutrition_scaled = pd.DataFrame(\n",
    "    train_nutrition_values,\n",
    "    columns=nutrition_df.columns,\n",
    "    index=train_nutrition.recipe_id\n",
    ")\n",
    "test_nutrition_scaled = pd.DataFrame(\n",
    "    test_nutrition_values,\n",
    "    columns=nutrition_df.columns,\n",
    "    index=test_nutrition.recipe_id\n",
    ")\n",
    "\n",
    "# merge nutrition back with main data\n",
    "train_df = pd.merge(train_df, train_nutrition_scaled, left_on='recipe_id', right_index=True, how='left')\n",
    "test_df = pd.merge(test_df, test_nutrition_scaled, left_on='recipe_id', right_index=True, how='left')\n",
    "\n",
    "# drop the date field before training\n",
    "if 'date' in train_df.columns:\n",
    "    train_df = train_df.drop('date', axis=1)\n",
    "    test_df = test_df.drop('date', axis=1)\n",
    "\n",
    "# final dataset shapes\n",
    "print(\"final train set shape:\", train_df.shape)\n",
    "print(\"final test set shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935030e3-8d46-47ac-9d48-f16e34bec421",
   "metadata": {},
   "source": [
    "**Encoding Tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2fc79-cb73-4330-9037-d98fe909e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode tags into separate rows\n",
    "tags_unnested = recipes_mlp.explode('tags')[['recipe_id', 'tags']]\n",
    "\n",
    "# get common tags based only on training data\n",
    "train_tags_unnested = tags_unnested[tags_unnested['recipe_id'].isin(train_df['recipe_id'])]\n",
    "tag_count = train_tags_unnested.groupby('tags').count().sort_values(by='recipe_id', ascending=False)\n",
    "common_tags = list(tag_count[tag_count.recipe_id > 50000].index)  # reduced threshold to capture more tags\n",
    "\n",
    "# filter and encode tags\n",
    "tags_filtered = tags_unnested[tags_unnested['tags'].isin(common_tags)]\n",
    "tags_encoded = pd.get_dummies(tags_filtered, columns=['tags'], prefix='', prefix_sep='')\n",
    "tags_encoded = tags_encoded.groupby('recipe_id').sum().reset_index()\n",
    "\n",
    "# add tags to train and test separately\n",
    "train_df = pd.merge(train_df, tags_encoded, on='recipe_id', how='left')\n",
    "test_df = pd.merge(test_df, tags_encoded, on='recipe_id', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e85bc-ff01-4182-91f7-9656262a19ac",
   "metadata": {},
   "source": [
    "**Handling NaN and Bias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390dc8d-6192-41e6-909f-fdf0d87c7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill nan values with median for each column (better than filling with 0)\n",
    "for col in train_df.columns:\n",
    "    if train_df[col].dtype in [np.float64, np.int64] and col not in ['user_id', 'recipe_id', 'rating', 'user_id_mapped', 'recipe_id_mapped']:\n",
    "        median_val = train_df[col].median()\n",
    "        train_df[col] = train_df[col].fillna(median_val)\n",
    "        test_df[col] = test_df[col].fillna(median_val)\n",
    "\n",
    "# calculate biases\n",
    "user_avg = train_df.groupby('user_id')['rating'].mean()\n",
    "recipe_avg = train_df.groupby('recipe_id')['rating'].mean()\n",
    "global_avg = train_df['rating'].mean()\n",
    "\n",
    "# add these as features\n",
    "train_df['user_bias'] = train_df['user_id'].map(user_avg) - global_avg\n",
    "test_df['user_bias'] = test_df['user_id'].map(user_avg).fillna(0)  # use 0 for unknown users\n",
    "\n",
    "train_df['recipe_bias'] = train_df['recipe_id'].map(recipe_avg) - global_avg\n",
    "test_df['recipe_bias'] = test_df['recipe_id'].map(recipe_avg).fillna(0)  # use 0 for unknown recipes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf7f1c-ea74-47d3-8728-fafc92297534",
   "metadata": {},
   "source": [
    "**Final Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f42364-6111-4fc9-9ff7-7324babc42f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop text columns not needed for modeling\n",
    "test_df = test_df.drop(['tags', 'ingredients'], axis=1)\n",
    "train_df = train_df.drop(['tags', 'ingredients'], axis=1)\n",
    "\n",
    "# prepare clean copy\n",
    "train_df_clean = train_df.copy()\n",
    "\n",
    "# filter test data to include only users and recipes in training data\n",
    "valid_users = set(train_df_clean['user_id'])\n",
    "valid_recipes = set(train_df_clean['recipe_id'])\n",
    "test_df_clean = test_df[test_df['user_id'].isin(valid_users) & test_df['recipe_id'].isin(valid_recipes)]\n",
    "print(f\"test data after filtering for valid users/recipes: {len(test_df_clean)} rows\")\n",
    "\n",
    "# extract user and recipe id features\n",
    "X_user_train = train_df_clean['user_id_mapped'].values\n",
    "X_recipe_train = train_df_clean['recipe_id_mapped'].values\n",
    "y_train = train_df_clean['rating'].values\n",
    "\n",
    "X_user_test = test_df_clean['user_id_mapped'].values\n",
    "X_recipe_test = test_df_clean['recipe_id_mapped'].values\n",
    "y_test = test_df_clean['rating'].values\n",
    "\n",
    "# extract other features, excluding ids and target\n",
    "feature_cols = [col for col in train_df_clean.columns \n",
    "                if col not in ['user_id', 'recipe_id', 'rating', 'user_id_mapped', 'recipe_id_mapped']]\n",
    "X_features_train = train_df_clean[feature_cols].values\n",
    "X_features_test = test_df_clean[feature_cols].values\n",
    "\n",
    "# final nan check and replacement\n",
    "X_features_train = np.nan_to_num(X_features_train, nan=0)\n",
    "X_features_test = np.nan_to_num(X_features_test, nan=0)\n",
    "\n",
    "# print final shapes\n",
    "print(\"training data shape:\", X_features_train.shape)\n",
    "print(\"testing data shape:\", X_features_test.shape)\n",
    "print(\"number of feature columns:\", len(feature_cols))\n",
    "print(\"feature names:\", feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4afd0a-7509-498f-bbcf-54ca216ace99",
   "metadata": {},
   "source": [
    "## 3. Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7314d-c87f-40cf-84c9-a5195d79b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"X_user_train dtype:\", X_user_train.dtype)\n",
    "print(\"X_recipe_train dtype:\", X_recipe_train.dtype)\n",
    "print(\"X_features_train dtype:\", X_features_train.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "\n",
    "# Convert arrays to proper numeric types\n",
    "X_user_train = X_user_train.astype(np.int32)\n",
    "X_recipe_train = X_recipe_train.astype(np.int32)\n",
    "X_features_train = X_features_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "X_user_test = X_user_test.astype(np.int32)\n",
    "X_recipe_test = X_recipe_test.astype(np.int32)\n",
    "X_features_test = X_features_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# Verify data shapes\n",
    "print(\"X_user_train shape:\", X_user_train.shape)\n",
    "print(\"X_recipe_train shape:\", X_recipe_train.shape)\n",
    "print(\"X_features_train shape:\", X_features_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bdf33f-7fc4-4037-9d52-80999f59e9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('X_user_train.csv', X_user_train, delimiter=',')\n",
    "np.savetxt('X_recipe_train.csv', X_recipe_train, delimiter=',')\n",
    "np.savetxt('X_features_train.csv', X_features_train, delimiter=',')\n",
    "np.savetxt('y_train.csv', y_train, delimiter=',')\n",
    "np.savetxt('X_user_test.csv', X_user_test, delimiter=',')\n",
    "np.savetxt('X_recipe_test.csv', X_recipe_test, delimiter=',')\n",
    "np.savetxt('X_features_test.csv', X_features_test, delimiter=',')\n",
    "np.savetxt('y_test.csv', y_test, delimiter=',')\n",
    "\n",
    "recipes_mlp.to_csv('recipes_mlp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
